# Вычисление многомерных интегралов с использованием многошаговой схемы (метод трапеций)

- Студент: Лопатин Артём Алексеевич, группа 3823Б1ПМоп3
- Технология: SEQ, MPI
- Вариант: 8

## 1. Введение
В настоящее время в профильных областях широко используются кластерные системы, упрощенно представляющие собой набор вычислительных узлов с индивидуальной памятью. Написание программ под такие системы (ввиду их специфики) требует организации межузловой коммуникации и синхронизации вычислений.

Средства межпроцессового взаимодействия описывает стандарт MPI (*Message Passing Interface* - интерфейс передачи сообщений). В учебных целях с использованием библиотеки (программной реализации стандарта) MPI была решена задача вычисления двумерных интегралов с использованием метода трапеций. Алгоритм задачи, сочетающий в себе достаточный объем вычислений на единицу данных и независимость итераций основного вычислительного цикла, хорошо распараллеливается и потенциально показывает существенный выигрыш в производительности.

## 2. Постановка задачи
Решается задача вычисления двумерных интегралов по прямоугольной области с использованием метода трапеций.

**Входные данные:** структура `IntegrationData`, содерщая границы (`double`) прямоугольной области `a` и `b` по x, `c` и `d` по y, интегрируемую функцию `f(x, y)` и размерность `n` равномерной по обеим осям квадратной сетки .  
**Выходные данные:** результат вычисления интеграла - число типа `double`.

**Требования:**
1. Реализация *последовательной* (**SEQ**) и *параллельной* (**MPI**) версий алгоритма.
2. Покрытие кода *функциональными* и *performance*-тестами (**gtest**).
3. Соблюдение заданной структуры репозитория и тестового интерфейса.

## 3. Описание базового алгоритма (последовательного)
Базовый алгоритм вычисления двумерного интеграла по прямоугольной области с использованием метода трапеций заключается в вычислении значений подынтегральной функции с весом 1 на внутренних узлах сетки, с весом 0.5 - на граничных узлах сетки и с весом 0.25 - на угловых. Каждое вычисленное значение есть высота части прямой криволинейной призмы-подграфика поверхности `f(x, y)`. Его умножение на площадь квадратного основания `h * k` позволяет найти объем соответвующей части подграфика с точностью до погрешности численного метода. Элементарные объемы вычисляются по всем узлам сетки, после чего аккумулируются в результирующей переменной `output`.

Следует отметить, что перед запуском вычислений входные данные проходят обязательную валидацию, которая заключается в проверке корректности границ прямоугольной области интегрирования (`a < b`, `c < d`), валидности подынтегральной функиции и размерности сетки (`n > 0`).

## 4. Схема распараллеливания
Параллельная реализация выполнена с использованием топологии стандартного коммуникатора `MPI_COMM_WORLD`. В качестве корневого процесса используется процесс с рангом 0. Он аккумулирует результаты частичных сумм (соответствуют интегралам на участках сетки) и выполняет обработку хвоста (в случае некратности числа узлов числу процессов).

Параллельный алгоритм включает в себя несколько этапов:
1. Входные данные инциализируются на всех процессах коммуникатора, вычисляются шаги сетки по обеим осям.
2. Вычисляются индексы `start` и `end`, разность которых есть размер сеточной порции на процесс (очевидно, зависит от числа процессов).
3. Каждый процесс выполняет вычисление по описанному в п. 3 отчета последовательному алгоритму на своей части сетки, сохраняя результат в `local_res`. Под частью сетки подразумевается участок сетки с порционным диапазоном узлов по x и полным диапазоном узлов по y.
4. Выполняется аккумуляция локальных результатов путем использования функции `MPI_Reduce` с параметром редуцирования `MPI_SUM`.
5. Обработкой хвоста (при его наличии) занимается процесс с рангом 0, вызывая соответствующую функцию (`double CalcTail(int, IntegrationData&, double, double)`). Результат добавляется к глобальному.
6. Результат вычисления рассылается всем процессам с помощью функции `MPI_Bcast` в целях синхронизации данных для прохождения тестов (тестовая инфраструктура требует наличия результата на всех процессах).

## 5. Детали реализации

### 5.1 Общий интерфейс (common/include)
В директории `common/include` находится заголовочный файл `common.hpp`, содержащий определение типов входных (`InType`) и выходных (`OutType`) данных, а также тип входных данных тестовых кейсов (`TestType`; в данном случае строка `int`, принимающий значение тест-кейса) и базовый класс интерфейса (`BaseTask`).

### 5.2 Формат тестовых данных (data)
Директория `data` пуста. Данные тестов генерируются программно.

### 5.3 Интерфейс реализаций алгоритмов (mpi & seq)
В директориях `mpi` и `seq` расположены `.hpp` и `.cpp` файлы соответствующих реализаций. Общий интерфейс включает методы валидации данных `bool ValidationImpl(void)` (алгоритм валидации описан в п. 3 отчета), предобработки данных `bool PreProcessingImpl(void)` (в задаче вычисления двумерного интеграла преобработка не требуется, функция обнуляет выходные данные и возвращает успех), вычисления значения интеграла `bool RunImpl(void)` (алгоритмы MPI и SEQ реализаций описаны в соответствующих пунктах отчета) и постобработки данных `bool PostProcessingImpl(void)` (в рассматриваемой задачи постобработка данных не требуется).

### 5.4 Тестирование (tests)
Директория `tests` содержит функциональные и *performance*-тесты с `main.cpp` реализациями в одноименных папках. Оба типа тестов имеют одинаковую структуру: в `void SetUp(void)` инициализации происходит инициализация тест-кейсов, по завершении вычислений методом `bool CheckTestOutputData(OutType& output_data)` производится проверка корректности результата путем сравнения с эталонным значением с поправкой на вычислительную погрешность машинной арифметики (оценки погрешности получены экспериментально). Для функционального тестирования предусмотрены 5 тестовых функций: полиномиальная (степенная), показательная (экспонента), тригонометрическая, логарифмическая и неберущаяся в элементарных функция экспоненты с квадратичной степенью. Производительность измеряется на вычислении интеграла от последней упомянутой функции на сетке большей размерности. Эталонные значения вычислены аналитически (для решаемых интегралов) и численно с использованием решателя WolframAlpha.

### 5.5 Управление памятью
Управление памятью осуществляется автоматически средствами библиотеки `STL`. Динамические указатели с ручным управлением памятью не используются.

## 6. Тестовая конфигурация
- Процессор: AMD Ryzen 5 3500x (Zen 2), 6 cores / 6 threads, 3.6 ГГц, Turbo 4.1 ГГц
- Память: 32 GB DDR4
- Операционная система: Windows 11 version 24H2
- Компилятор: clang 19.44.35216, ключи: /O2
- Реализация MPI: MS-MPI version 10.1.12498.52
- Параметры запуска: `mpiexec -n <count>`
- Данные: *performance*-тест на сетке размерностью `n = 20000`

## 7. Экспериментальные результаты

### 7.1 Корректность
Проверка корректности осуществлялась путем сравнения с эталонным значением, в качестве которого был принят результат вычисления интеграла сервисом WolframAlpha. Допускалась погрешность машинной арифметики порядка 1e-4.

### 7.2 Производительность

| Mode        | Count | Time, s  | Speedup | Efficiency |
|-------------|-------|----------|---------|------------|
| seq         | 1     | 2.65     | 1.00    | N/A        |
| mpi         | 2     | 1.5503   | 1.71    | 85.5%      |
| mpi         | 4     | 0.8123   | 3.26    | 81.5%      |
| mpi         | 6     | 0.6774   | 3.91    | 65.2%      |
| mpi         | 8     | 0.5984   | 4.43    | 55.3%      |

Полученные результаты показывают ускорение MPI-реализации вычисления двумерного интеграла на машине с общей памятью. С ростом числа процессов время работы программы уменьшается, однако эффективность параллельной реализации падает. На 6 и 8 процессах ускорение далеко от теоретической границы. Наилучшая эффективность **85.5%** достигнута при использовании 2 процессов.
Результаты эксперимента демонстрируют хорошее ускорение вычислений при использовании параллельной реализации, однако накладные расходы MPI остаются существенными.

## 8. Заключение
В результате работы в учебных целях разработаны последовательная (SEQ) и параллельная (MPI) версии программы, вычисляющей двумерный интеграл по прямоугольной области методом трапеций.
Проведенный в процессе выполнения работы вычислительный эксперимент продемонстрировал улучшение производительности при использовании параллельной MPI-реализации алгоритма. Эффективность варьировалась в пределах от 85.5% (на 2 процессах) до 55.3% (на 8 процессах).

## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 24.12.2025).
2. WolframAlpha [Электронный ресурс] // URL: https://www.wolframalpha.com/ (дата обращения: 24.12.2025).