# Гиперкуб

- Student: Черных Севастьян Владимирович, group 3823Б1ПМоп3
- Technology: <SEQ | MPI>
- Variant: 10
## 1. Introduction
Требуется изучить принципы обмена данными в топологии «гиперкуб», которая обеспечивает логарифмическое время связи между узлами.

## 2. Problem Statement
На вход поступает вектор положительных чисел, список "активных" узлов (целые числа — ранги).

Количество процессов в системе (должно быть степенью двойки).

Целью работы является реализация алгоритма вычисления суммы (или другой ассоциативной операции) значений, распределенных по узлам вычислительной системы, организованной в топологию гиперкуба. Программа должна корректно работать на системах, где количество процессов p=2d, где d — размерность гиперкуба.

## 3. Baseline Algorithm (Sequential)
Поскольку гиперкуб размерности 0 не имеет смысла, seq код реализован в виде заглушки и не тестируется.
## 4. Parallelization Scheme
На каждом шаге i (измерение гиперкуба) сосед вычисляется через rank ^ (1 << i). Это соответствует узлу, номер которого отличается от текущего ровно в одном бите. 
Условие (rank & (mask - 1)) != 0 отсекает узлы, которые уже передали свои данные на предыдущих этапах и больше не участвуют в сборке.
Если у процесса установлен бит текущего измерения (rank & mask), он отправляет данные (MPI_Send) и завершает работу.
Если бит не установлен, он принимает данные (MPI_Recv) от соседа и прибавляет их к своей локальной сумме.
Количество итераций обмена равно log2​(size), что является оптимальным для данной топологии.
## 5. Performance

Измерения проводились на ОС Ubuntu (WSL) с использованием флага `--oversubscribe` для запусков, превышающих количество физических ядер.
Так же была добавлена искусственная нагрузка, чтобы значение T было не слишком маленьким.

| Тип реализации | Процессы (P) | Время T (сек) | Ускорение S | Эффективность E |
| -------------- | ------------ | ------------- | ----------- | --------------- |
| MPI            | 2            | 0.07861       | 1.00        | 100%            |
| MPI            | 4            | 0.07725       | 1.02        | 25.5%           |
| MPI            | 8            | 0.08509       | 0.92        | 11.5%           |
| MPI            | 16           | 0.17280       | 0.45        | 2.8%            |


## 6. Conclusions
Реализован алгоритм параллельной редукции в топологии «гиперкуб». Программа успешно проходит функциональные тесты, выполняя сборку данных за log2​P шагов.    
Замеры на 2, 4, 8 и 16 процессах показали рост временных затрат. Это обусловлено доминированием накладных расходов MPI над микроскопическим объемом вычислений (сложение чисел).
Подтверждена работоспособность двоичной маршрутизации данных и масштабируемость логики обменов.

