# Звезда

- Студент: Лопатин Артём Алексеевич, группа 3823Б1ПМоп3
- Технология: MPI
- Вариант: 8

## 1. Введение
В настоящее время в профильных областях широко используются кластерные системы, упрощенно представляющие собой набор вычислительных узлов с индивидуальной памятью. Написание программ под такие системы (ввиду их специфики) требует организации межузловой коммуникации и синхронизации вычислений.

Средства межпроцессового взаимодействия описывает стандарт MPI (*Message Passing Interface* - интерфейс передачи сообщений). В учебных целях с использованием библиотеки (программной реализации стандарта) MPI была решена задача организации обмена данными в соответствии с топологией "звезда". Данная топология предполагает наличие одного центрального узла и множества прилегающих (периферийных) узлов, каждый из которых способен коммуницировать только с "центром". Такая структура обмена данными часто бывает полезна, в частности, при организации совместной работы с разделяемыми данными.
Следует отметить, что задача носит ознакомительный характер, поскольку MPI предлагает встроенные средства разворачивания топологий коммуникатора `MPI_Cart_Create` и `MPI_Graph_Create`.

## 2. Постановка задачи
Решается задача задача организации межпроцессового ваимодействия (обмена данными) в соответствии с топологией коммуникатора "звезда".

**Входные данные:** кортеж `std::tuple<int, int, int, std::vector<double>>`, содержащий ранги отправителя и получателя, размер передаваемого сообщения и данные сообщения типа `double`. 
**Выходные данные:** полученное сообщение `std::vector<double>`.

**Требования:**
1. Реализация *параллельной* (**MPI**) версии алгоритма без использования встроенных средств **MPI** по созданию топологий коммуникатора.
2. Покрытие кода *функциональными* и *performance*-тестами (**gtest**).
3. Соблюдение заданной структуры репозитория и тестового интерфейса.

## 3. Описание последовательного алгоритма
Последовательная реализация представляет собой заглушку, возвращающую `true` на всех этапах. Тестовый интерефейс не позволяет без внесения корректировок выполнить вызов MPI-функций внутри **SEQ** версии. 

## 4. Схема распараллеливания
Параллельная реализация выполнена с использованием топологии стандартного коммуникатора `MPI_COMM_WORLD`. Алгоритм предполагает построение структуры связей поверх стандартной топологии путем ограничения прямого взаимодействия между отдельными процессами (т.н. **листами**). В качестве центрального узла топологии используется процесс с рангом 0.

Параллельный алгоритм включает в себя несколько условных веток:
1. Вырожденный случай: ранги отправителя и получателя совпадают. Коммуникаций не требуется.
2. Случай коммуникации с центральным процессом. В этом случае организуется парное взаимодействие с помощью функций `MPI_Send` и `MPI_Recv`. Алгоритм тривиален.
3. Случай коммуникации двух нецентральных процессов (листьев). Обмен данными происходит в два этапа: процесс-отправитель выполняет `MPI_Send` на центральный процесс, который резервирует буфферную память, получает сообщение отправителя через `MPI_Recv` и перенаправляет его на процесс-получатель посредством вызова `MPI_Send`. Процесс-получатель выполняет `MPI_Recv`. Владельцами данных оказываются все процессы цепочки.

## 5. Детали реализации

### 5.1 Общий интерфейс (common/include)
В директории `common/include` находится заголовочный файл `common.hpp`, содержащий определение типов входных (`InType`) и выходных (`OutType`) данных, а также тип входных данных тестовых кейсов (`TestType`; в данном случае строка `std::string`, представляющая имя файла с тестовыми данными) и базовый класс интерфейса (`BaseTask`).

### 5.2 Формат тестовых данных (data)
Директория `data` содержит текстовые файлы `.txt` с маской имени `func_<type-of-communication>`. В файлах записаны данные (первые две строки содержат ранги отправителя и получателя соответственно, третья строка - размер сообщения (количество элементов передаваемых данных), четвертая - набор элементов сообщения) для *функциональных* тестов. Бинарный файл `perf_leaf_to_leaf.bin` содержит тестовую информацию для *performance*-тестов в формате `<src-rank == 3><dst-rank == 2><msg-size == 8388608><vector-data>`.

Данные для всех тестов сгенерированы в отдельном приложении с использованием генератора псевдо-случайных чисел типа *Mersenne Twister* `std::mt19937` по равномерному распределению `std::uniform_real_distribution` в отрезках $[-15.0, 15.0]$ (для функциональных тестов) и $[-64.0, 64.0]$ (для тестов производительности).

### 5.3 Интерфейс реализаций алгоритмов (mpi & seq)
В директориях `mpi` и `seq` расположены `.hpp` и `.cpp` файлы соответствующих реализаций. Общий интерфейс включает методы валидации данных `bool ValidationImpl(void)` (проверка непустоты входных данных и корректности рангов отправителя и получателя), предобработки данных `bool PreProcessingImpl(void)` (в задаче построения топологии предобработка данных не требуется, функция обнуляет выходные данные и возвращает успех), организации топологии коммуникатора "звезда" `bool RunImpl(void)` (алгоритм MPI реализации описан в п. 4 отчета) и постобработки данных `bool PostProcessingImpl(void)` (в задаче построения топологии постобработка данных не требуется).

### 5.4 Тестирование (tests)
Директория `tests` содержит функциональные и *performance*-тесты с `main.cpp` реализациями в одноименных папках. Оба типа тестов имеют одинаковую структуру: в `void SetUp(void)` инициализации происходит чтение входных данных из файла, по завершении обмена данными методом `bool CheckTestOutputData(OutType& output_data)` производится проверка корректности результата путем сравнения данных полученного сообщения с инициализированными на этапе `SetUp` данными.

### 5.5 Управление памятью
Управление памятью осуществляется автоматически средствами контейнера `std::vector`. Динамические указатели с ручным управлением памятью не используются.

## 6. Тестовая конфигурация
- Процессор: AMD Ryzen 5 3500x (Zen 2), 6 cores / 6 threads, 3.6 ГГц, Turbo 4.1 ГГц
- Память: 32 GB DDR4
- Операционная система: Windows 11 version 24H2
- Компилятор: clang 19.44.35216, ключи: /O2
- Реализация MPI: MS-MPI version 10.1.12498.52
- Параметры запуска: `mpiexec -n <count>`
- Данные: векторы размерности `16777216`, сгенерированные в соответствие с п. 5.2. и размещенные на локальном SSD-накопителе

## 7. Экспериментальные результаты

### 7.1 Корректность
Проверка корректности осуществлялась путем сравнения полученного сообщения с исходными данными отправителя и проверки наличия данных в буфферной памяти центрального процесса.

### 7.2 Производительность

|       Mode       | Count | Time, ms | Speedup | Efficiency |
|------------------|-------|----------|---------|------------|
| MPI_Graph_Create | 4     | 157.20   | 1.00    | N/A        |
| mpi              | 4     | 168.49   | 0.93    | 93%        |

Полученные результаты показывают небольшое **замедление** ручной реализации топологии "звезда" MPI-коммуникатора. Выигрыш в производительности стандартных средств создания топологий объясняется лучшей оптимизацией межпроцессового взаимодействия в тестируемой реализации MPI.

## 8. Заключение
В результате работы в учебных целях разработана параллельная (MPI) версии программы, выполняющей создание топологии коммуникатора типа "звезда".
Проведенный в процессе выполнения работы эксперимент продемонстрировал незначительное снижение производительности рукописной версии. Эффективность представленной реализации с учетом округления составила 93% на 4 процессах.

## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 21.12.2025).